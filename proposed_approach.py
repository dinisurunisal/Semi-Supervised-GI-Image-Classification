# -*- coding: utf-8 -*-
"""Proposed Approach.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KazBCMnNI9zR7UB-kjhItFzXT7heo75m
"""

!nvidia-smi

from google.colab import drive
drive.mount('/content/drive')

"""# Import Main Libraries"""

pip install -q -U tensorflow-addons

import tensorflow as tf
import tensorflow_addons as tfa
import numpy as np
import pathlib
import matplotlib.pyplot as plt
import datetime
import math
import itertools 
import pickle
import time
import os
import sys
import shutil
import keras
import glob
import cv2
import random
import textwrap
import scipy.ndimage as ndimage
import seaborn as sns

from sklearn.model_selection import train_test_split
from math import ceil, floor
from pathlib import Path
from tqdm import tqdm
from sklearn import metrics
from tqdm.notebook import tqdm
from IPython.display import clear_output
from PIL import Image, ImageDraw, ImageFont

"""# Setup Colab"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2
# %matplotlib inline

"""# Unzip Datasets"""

!ls "/content/drive/My Drive/ColabNotebooks/Datasets"
!unzip "/content/drive/My Drive/ColabNotebooks/Datasets/hyperkvasir-labelled-data.zip"
!unzip "/content/drive/MyDrive/ColabNotebooks/Datasets/hyper-kvasir-unlabeled-images.zip"

"""# Initialize"""

labeled_data_root = pathlib.Path('/content/hyperkvasir-labelled-data')
unlab_data_root = pathlib.Path('/content/unlabeled-images')

# Primary directory
record_dir = "./records/{}".format("labelled_model")
pathlib.Path(record_dir).mkdir(parents=True, exist_ok=True) # record_directory

cache_dir = "./cache"
pathlib.Path(cache_dir).mkdir(parents=True, exist_ok=True) # cache directory

"""# Split Dataset"""

# Split the dataset into train, test and validation splits
def split_dataset():

    # The seed() method is used to initialize the random number generator.
    random.seed(1500)
    
    # listdir() returns a list containing the names of the entries in the directory
    class_names = os.listdir(labeled_data_root)
    image_folders = [os.path.join(labeled_data_root, class_name) for class_name in class_names]
    
    num_samples = len(list(Path(labeled_data_root).glob('*/*')))

    # Create folders for each class under the train, test and val folders
    train_folders = [os.path.join('labeled_data', 'train', name) for name in class_names]
    test_folders = [os.path.join('labeled_data', 'test', name) for name in class_names]
    val_folders = [os.path.join('labeled_data', 'val', name) for name in class_names]

    for folder in train_folders:
      Path(folder).mkdir(parents=True, exist_ok=True)

    for folder in test_folders:
      Path(folder).mkdir(parents=True, exist_ok=True)
                              
    for folder in val_folders:
      Path(folder).mkdir(parents=True, exist_ok=True)

    # tqdm is the library which was used to create the Progress Bar
    tqdm_img = tqdm(total=num_samples, desc='Splitting Progress', position=0)

    # split all the classes equally to test, train and val
    for index, directory in enumerate(image_folders):

        random.seed(1500)
        imagenames = os.listdir(directory)
        imagenames.sort() # make sure that the image filenames have a fixed order before shuffling
        random.shuffle(imagenames) # shuffles the ordering of image filenames (deterministic given the chosen seed)
        num_samples = len(imagenames)

        # Here the dataset splits according to the given float values
        imagenames = np.array(imagenames)
        imagenames_split = np.split(imagenames, [floor(num_samples*float(0.7)), floor(num_samples*float(0.85))]) #Train: 70%| Test: 15% | Val: 15%
        
        dataset = {'train': imagenames_split[0],
                   'test': imagenames_split[1],
                   'val': imagenames_split[2]}

        for split in dataset:
            output = os.path.join('labeled_data', split, directory.split("/")[-1])
            for filename in dataset[split]:
                filename = os.path.join(directory, filename)
                image_resize(filename, output) # Resize the image to 128, 128 dimensions
                tqdm_img.update(1) #Updates the Progress Bar

# Method used to resize the image to 128 size
def image_resize(filename, output_dir):
    image = cv2.imread(filename, cv2.IMREAD_UNCHANGED)
    image = cv2.resize(image, (128, 128), interpolation=cv2.INTER_LINEAR)
    path = os.path.join(output_dir, filename.split('/')[-1])
    cv2.imwrite(path, image) # Save the image in the output directory

split_dataset()

BATCH_SIZE = 8 # Hyperparameter
CLASS_NAMES = None
NUMBER_CLASSES = 0
DATASET_SIZE_TRAIN = None
DATASET_SIZE_TEST = None
DATASET_SIZE_VAL = None
DATASET_SIZE_UNLAB = None
STEPS_TRAIN = None
STEPS_TEST = None
STEPS_VAL = None
RAW_TRAIN_DATASET = None
AUTOTUNE = tf.data.experimental.AUTOTUNE

# function to load input image, preprocess it and get label
def load_label_images(image_path):
    # read the image from database, decode it, convert the data type to
	  # floating point, and then resize it
    image_raw = tf.io.read_file(image_path)
    image = tf.image.decode_jpeg(image_raw, channels=3)
    image = tf.image.convert_image_dtype(image, tf.float32)
    image = tf.image.resize(image, [128, 128])

    # parse the class label from the file path
    str_targets = tf.strings.split(image_path, os.path.sep)[-2]
    label_integer = tf.reduce_min(tf.where(tf.equal(str_targets, CLASS_NAMES)))
    label = tf.dtypes.cast(label_integer, tf.int32)

    return image, label

# Method used to organize the datasets, making it suitable to train
def build_datasets():

    global CLASS_NAMES
    global NUMBER_CLASSES
    global DATASET_SIZE_TRAIN
    global DATASET_SIZE_TEST
    global DATASET_SIZE_VAL
    global STEPS_TRAIN
    global STEPS_TEST
    global STEPS_VAL
    global RAW_TRAIN_DATASET

    labeled_data_root = pathlib.Path('/content/labeled_data')
    np.random.seed(seed=1500)

    class_names = np.array([item.name for item in labeled_data_root.glob('train/*')])

    CLASS_NAMES = class_names
    NUMBER_CLASSES = len(class_names)

    model_class_names = list(class_names)
    handle = open(record_dir+"/model_class_names.pkl", 'wb')
    pickle.dump(model_class_names, handle)
    
    dataset = {'train': str(labeled_data_root/'train'/'*/*.*g'),
          'test': str(labeled_data_root/'test'/'*/*.*g'),
          'val': str(labeled_data_root/'val'/'*/*.*g')}

    DATASET_SIZE_TRAIN = len(list(glob.glob(dataset["train"])))
    DATASET_SIZE_TEST = len(list(glob.glob(dataset["test"])))
    DATASET_SIZE_VAL = len(list(glob.glob(dataset["val"])))
    
    for split in dataset:
        dataset[split] = tf.data.Dataset.list_files(dataset[split],
                                    shuffle=True,
                                    seed=tf.constant(1500, tf.int64)
                                    )
    
    # Map labels with the images from tf.dataset to create "train", "test", "val" datasets
    for split in dataset:
        dataset[split] = dataset[split].map(load_label_images, num_parallel_calls=AUTOTUNE)
    
    RAW_TRAIN_DATASET = dataset["train"] # Clean dataset of the "train" split
    
    print("[INFO] loading the dataset...")

    # training dataset split will go through a resample, suffle, repeat, data augment,
    # and prefetched phase
    dataset["train"] = resample(dataset["train"])
    dataset["train"] = (
      dataset["train"]
      .shuffle(buffer_size=3000, seed=tf.constant(1500, tf.int64))
      .repeat()
      .map(dataset_augmentation(), num_parallel_calls=AUTOTUNE)
      .batch(BATCH_SIZE, drop_remainder=False)
      .prefetch(buffer_size=AUTOTUNE)
    )

    # testing dataset split will go through a caching, suffle, repeat,
    # and prefetched phase
    cache_string = "{}/{}_{}".format(cache_dir, 128, "test")
    dataset["test"] = (
      dataset["test"]
      .cache(cache_string)
      .shuffle(buffer_size=3000, seed=tf.constant(1500, tf.int64))
      .repeat()
      .batch(BATCH_SIZE, drop_remainder=False)
      .prefetch(buffer_size=AUTOTUNE)
    )

    # testing dataset split will go through a caching, suffle, repeat,
    # and prefetched phase
    cache_string = "{}/{}_{}".format(cache_dir, 128, "val")
    dataset["val"] = (
      dataset["val"]
      .cache(cache_string)
      .shuffle(buffer_size=3000, seed=tf.constant(1500, tf.int64))
      .repeat()
      .batch(BATCH_SIZE, drop_remainder=False)
      .prefetch(buffer_size=AUTOTUNE)
    )

    print("[INFO] train, test and val datasets created")

    STEPS_TRAIN = DATASET_SIZE_TRAIN//BATCH_SIZE
    STEPS_TEST = DATASET_SIZE_TEST//BATCH_SIZE
    STEPS_VAL = DATASET_SIZE_VAL//BATCH_SIZE
    
    return dataset

# Method used to count the sample count in a class
def count_class_samples(dataset, num_classes):
    count = np.zeros(num_classes) # returns a new array with zeros according to the class count
    for img, lab in dataset:
        count[lab] += 1
    return count

"""Resample Dataset"""

# Method used to resample the dataset. This is a solution of highly imbalanced problem
def resample(dataset):

    datasets = []
    cache_dir = './cache/{}_resampled_{}/'.format(128, 'train')
    pathlib.Path(cache_dir).mkdir(parents=True, exist_ok=True) # create cache directory
    
    for i in range(NUMBER_CLASSES): # NUMBER_CLASSES = 23
        data = dataset.filter(lambda img, lab: lab==i)
        data = data.cache(cache_dir+'{}_dataset'.format(i))
        
        data = data.repeat()
        datasets.append(data)
    
    target_dist = [ 1.0/NUMBER_CLASSES ] * NUMBER_CLASSES
    # samples elements from the datasets in the array datasets at randomly
    balanced_dataset = tf.data.experimental.sample_from_datasets(datasets, weights=target_dist, seed=1500)
    
    return balanced_dataset

"""Data Augmentation"""

# Method to image augment the dataset
def dataset_augmentation():

    tf.random.set_seed(1500) # global seed
    
    def image_augment(image, label):

        degree = tf.random.normal([])*360
        image = tfa.image.rotate(image, degree * math.pi / 180, interpolation='BILINEAR')
        image = tf.image.resize_with_crop_or_pad(image, 138, 138)
        image = tf.image.random_crop(image, (128, 128, 3), seed=1500)
        image = tf.image.random_brightness(image, max_delta=0.2, seed=1500)
        image = tf.image.random_saturation(image, lower=0.5, upper=1.5, seed=1500)
        image = tf.image.random_hue(image, max_delta=0.2, seed=1500)
        image = tf.image.random_contrast(image, lower=0.5, upper=1.5, seed=1500)
        image = tf.image.random_flip_left_right(image, seed=1500)
        image = tf.image.random_flip_up_down(image, seed=1500)

        # The random_* ops do not necessarily clamp.
        # Here it clip tensors value to min 0 and max 1
        image = tf.clip_by_value(image, 0.0, 1.0)
        return image, label
    
    return image_augment

dataset = build_datasets()

"""# Load unlabelled dataset"""

# Method to load the unlabelled data
def load_unlab_images(image_path):
    # read the image from database, decode it, convert the data type to
	  # floating point, and then resize it
    image = tf.io.read_file(image_path)
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.convert_image_dtype(image, tf.float32)
    image = tf.image.resize(image, [128, 128])

    # obtain filename
    filename = tf.strings.split(image_path, os.path.sep)[-1]
    return image, filename

def create_unlab_dataset():

  global DATASET_SIZE_UNLAB

  unlab_data_root = pathlib.Path('/content/unlabeled-images/images')

  DATASET_SIZE_UNLAB = len(list(unlab_data_root.glob('*.*g')))

  files_string = str(unlab_data_root/'*.*g')
  list_dataset_unlabeled = tf.data.Dataset.list_files(
          files_string, 
          shuffle=True, 
          seed=tf.constant(1500, tf.int64)
  )
  
  unlab_dataset = list_dataset_unlabeled.map(load_unlab_images, num_parallel_calls=AUTOTUNE)

  print ("Loaded {} images into unlabeled_dataset.".format(DATASET_SIZE_UNLAB))
  
  return unlab_dataset

dataset["unlab"] = create_unlab_dataset()

"""# Define the Hybrid Ensemble model used for labelled data"""

from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras.models import Sequential, Model, load_model
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, GlobalMaxPool2D, Multiply, Concatenate, Input, Dropout, ZeroPadding2D, Flatten
from tensorflow.keras import backend as K
from keras.layers import concatenate

datasets_bin = [count_class_samples( RAW_TRAIN_DATASET, NUMBER_CLASSES)]
dataset["mix_train"] = RAW_TRAIN_DATASET

# Method to define all the algorithms used for the architecture
def define_all_models():
    all_models = list()

    inputs = Input(batch_shape=(None, None, None, 3))

    base_model1 = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(
        weights='imagenet', 
        include_top=False, 
        input_shape=(128, 128, 3),
        input_tensor=inputs
    )

    all_models.append(base_model1)

    base_model2 = tf.keras.applications.resnet50.ResNet50(
        weights='imagenet', 
        include_top=False, 
        input_shape=(128, 128, 3),
        input_tensor=inputs
    )

    all_models.append(base_model2)
    return all_models, inputs

# Create the final architecture
def model_define():

    models, inputs = define_all_models()

    for i in range(len(models)):
      model = models[i]
      for layer in model.layers:
        layer._name = 'ensemble_' + layer.name + str('_model_') + str(i + 1)
    
    print(models[0].output.shape)
    print(models[1].output.shape)

    out1 = models[0].output
    out1 = ZeroPadding2D(((0,2), (0,2)))(out1) # Zero-padding layer for 2D input 
    
    outputs = concatenate([out1, models[1].output])

    # used for global average pooling operation for spatial data.
    z = GlobalAveragePooling2D()(outputs) 
    z = Flatten()(z) # newly added
    z = Dropout(0.3)(z)  # to prevent overfitting
    z = Dense(28, activation='relu')(z) # relu to overcome gradient decent problem 
    z = Dense(3584, activation='sigmoid')(z)
    z = Multiply()([outputs, z])
    z = GlobalAveragePooling2D()(z)
    z = Dense(NUMBER_CLASSES, activation="softmax")(z) # "Softmax" Activation layer for classification

    model = Model(inputs=inputs, outputs=z) 

    # plot_model(model, show_shapes=True, to_file='ensemble_models.png')
    
    model.compile(
        optimizer=Adam(learning_rate=0.001), # Good with sparse data
        loss='sparse_categorical_crossentropy',
        metrics=['sparse_categorical_accuracy']
    )
    
    model.summary()
    
    return model

model = model_define()

"""# Train the model for labelled data"""

start = time.time()

model_train = model.fit(
        dataset["train"],
        steps_per_epoch = STEPS_TRAIN,
        epochs = 15,
        validation_data = dataset["val"],
        validation_steps = STEPS_VAL,
        validation_freq = 1,
        verbose = 1
)

print ("Total time:", (np.round(time.time() - start, 4)/60), "minutes.")

print(model_train.history.keys())

def save_model():
  handle = open(record_dir+"/model_train.pkl", 'wb')
  pickle.dump(model_train.history, handle)
  model.save(record_dir+'/model.h5')

"""## Model Evaluation"""

def calculate_true_and_predicted_cat(model, dataset):

  # Using test data to evaluate the model
  test_evaluate = model.evaluate(dataset["test"], steps=STEPS_TEST) # steps since tf.dataset
  print("\nTest Accuracy: ", test_evaluate)

  # Find true_categories and predicted_categories
  eval_dataset = dataset["test"].unbatch().take(DATASET_SIZE_TEST)
  eval_dataset = eval_dataset.as_numpy_iterator()
  eval_dataset = np.array(list(eval_dataset))
  true_categories = list(eval_dataset[:,1]) 
  eval_images = np.stack(eval_dataset[:,0], axis=0) # eval_dataset[:,0] = give me the 0th index of all the rows in eval_dataset

  # Create predictions and predicted_categories
  predictions = model.predict(eval_images)
  predicted_categories = [np.argmax(pred) for pred in predictions]

  return true_categories, predicted_categories

def using_evaluation_metrics(model_train, true_categories, predicted_categories):

    sns.set()
    
    # Plot Accuracy
    plot_accuracy(model_train)
    # Plot Loss
    plot_loss(model_train)

    print('\nAccuracy score is :', metrics.accuracy_score(true_categories, predicted_categories))
    print('Precision score is :', metrics.precision_score(true_categories, predicted_categories, average='weighted'))
    print('Recall score is :',metrics.recall_score(true_categories, predicted_categories, average='weighted'))
    print('F1 Score is :', metrics.f1_score(true_categories, predicted_categories,average='weighted'))
    print('Cohen Kappa Score :', metrics.cohen_kappa_score(true_categories, predicted_categories))
    print("MCC :", metrics.matthews_corrcoef(true_categories, predicted_categories))
    
    # Print the Classification report
    print("\nClassification Report: ")
    report = metrics.classification_report( y_true=true_categories, y_pred=predicted_categories,
                labels=range(NUMBER_CLASSES),
                target_names=CLASS_NAMES,
                digits=3,
                zero_division=0)
    print(report)

def plot_confusion_matrix(cm, names=None):

    print('Confusion Matrix')
    
    plt.figure(figsize=(20,20))

    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('Confusion matrix')
    plt.colorbar()

    tick_marks = np.arange(NUMBER_CLASSES)
    plt.xticks(tick_marks, CLASS_NAMES, rotation=90)
    plt.yticks(tick_marks, CLASS_NAMES)

    # normalize:
    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    cm = np.around(cm, decimals=2)
    cm[np.isnan(cm)] = 0.0
    print("Normalized confusion matrix")

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")
    plt.tight_layout()
    plt.ylabel('True Categories')
    plt.xlabel('Predicted Categories')
    plt.savefig(record_dir+"/confusion_matrix.pdf", format="pdf")
    plt.show()

def plot_accuracy(model_train):
  
  plt.plot(model_train.history['sparse_categorical_accuracy'], label='Training Accuracy')
  plt.plot(model_train.history['val_sparse_categorical_accuracy'], label='Validation Accuracy')
  plt.title('Model Accuracy')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(loc='upper left')
  plt.show()

def plot_loss(model_train):
  plt.plot(model_train.history['loss'], label='Training Loss')
  plt.plot(model_train.history['val_loss'], label='Validation Loss')
  plt.title('Model Loss')
  plt.ylabel('Loss')
  plt.xlabel('Epoch')
  plt.legend(loc='upper left')
  plt.show()

true_categories, predicted_categories = calculate_true_and_predicted_cat(model, dataset)
using_evaluation_metrics(model_train, true_categories, predicted_categories)

# Plot the Confusion Matrix
cm = metrics.confusion_matrix(true_categories, predicted_categories)
plot_confusion_matrix(cm, CLASS_NAMES)

save_model()

"""# Generate pseudo labels on unlabeled images using the previously built model

### Generate pseudo labels for the unlabelled dataset
"""

findings = {
    "prediction_value_list": [], 
    "label_index_list": [], 
    "img_name_list": []
    }

def generate_pseudo_labels(unlab_dataset, model):

    # Use the unlabeled dataset to generate new psuedo labels
    start = time.time()
    findings_count = 0

    # Show tqdm bars
    predicting_bar = tqdm(total=DATASET_SIZE_UNLAB, desc='Predicting', position=0, initial=0)
    findings_bar = tqdm(total=DATASET_SIZE_UNLAB, desc='Findings', position=1, bar_format='{desc}:{bar}{n_fmt}', initial=0)
    
    # Iterates the images one by one
    for tot_cnt, (image,path) in enumerate(unlab_dataset, start=0):
      if tot_cnt > 20000:
          break
              
      img = np.expand_dims(image, 0)
      prediction = model.predict(img) # returns numpy array(s) of predictions.
      highest_pred = np.max(prediction) # returns the maximum value

      # Check if the highest_pred is more than the threshold value
      if highest_pred > 0.85:
          prediction_index = np.argmax(prediction).astype(np.uint8) # returns the index of the max element

          findings["label_index_list"].append(prediction_index)
          findings["prediction_value_list"].append(highest_pred)
          findings["img_name_list"].append(path)
              
          findings_count += 1
          findings_bar.update(1)
      predicting_bar.update(1)

    # Save the pseudo labels
    handle = open(record_dir+"/pseudo_labels.pkl", 'wb')
    pickle.dump(findings, handle)
        
    print ("\nTotal run time: {:.1f} min.".format( (time.time() - start)/60 ))
    print ("Found {} new samples in unlabeled_dataset after looking at {} images.".format(findings_count, tot_cnt))
        
    return findings

findings = generate_pseudo_labels(dataset["unlab"], model)

"""### Sort the lists from confidence levels highest to lowest


"""

sorted_list = list(zip(findings["prediction_value_list"], findings["label_index_list"], findings["img_name_list"]))
sorted_list.sort(key=lambda x: x[0], reverse=True)

pred_sorted = [row[0] for row in sorted_list]
lab_sorted = [row[1] for row in sorted_list]
name_sorted = [row[2] for row in sorted_list]

pseudo_sorted = {
    "prediction_value_list": pred_sorted,
    "label_index_list": lab_sorted,
    "img_name_list": name_sorted
}

"""### Create new dataset by combining train data and new findings"""

unlab_data_root = pathlib.Path('/content/unlabeled-images/images')

def create_new_dataset(dataset, pseudo_sorted, datasets_bin):

  new_findings, added_samples = resample_new_dataset(pseudo_sorted, datasets_bin[-1])

  # create a Dataset whose elements are slices of the new findings
  findings_tensor = tf.data.Dataset.from_tensor_slices(new_findings)
  dataset["mix_train"] = dataset["mix_train"].concatenate(findings_tensor) # Combine the train dataset with new findings

  # count class samples of the combined dataset
  datasets_bin.append(count_class_samples(dataset["mix_train"], NUMBER_CLASSES))

  return datasets_bin, added_samples

def read_unlab_images(tensor, folder):
    # function to read unlabelled images
    
    tensor = tensor.numpy().decode("utf-8")
    image = tf.io.read_file("{}/{}".format(folder, tensor))
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.convert_image_dtype(image, tf.float32)
    image = tf.image.resize(image, [128, 128])
    return image

# This is done because images should be added to classes equally without adding hundreds of images to
# one class, and nothing for another
def resample_new_dataset(pseudo, initial_class_distrib):

    # Add the new findings to respective classes
    added = {}
    newly_added_total = 0
        
    print ('Resample')
    print ("-"*50)

    latest_additions = ([], [])
    latest_additions_filepaths = []

    # Convert the input to an array.
    label_array = np.asarray(pseudo["label_index_list"], dtype=np.uint8) 

    for class_index in range(NUMBER_CLASSES):

        # check the number of samples currently in the class
        sample_count = initial_class_distrib[class_index]

        indexes = np.where(label_array == class_index)[0]
        num_latest_additions = len(indexes)

        count = 0
        for count, i in enumerate(indexes, start=1):
            if sample_count >= 1000:
                count -= 1 # one count should be reduced as it is already added using enumerate()
                break
            tensor = pseudo["img_name_list"][i]
            image = read_unlab_images(tensor, unlab_data_root)
            
            # append the images
            latest_additions[0].append(image)  
            # append the labels of the images                        
            latest_additions[1].append(pseudo["label_index_list"][i])     
            # append the filepaths of the images     
            latest_additions_filepaths.append(pseudo["img_name_list"][i])
            sample_count += 1
            
        newly_added_total += count

        print ("{:30}: newly added {} samples out of {} samples".format(CLASS_NAMES[class_index], count, num_latest_additions))
            
        added[CLASS_NAMES[class_index]] = [count, num_latest_additions]
    
    print ("-"*50)
    print ("Total number of {} samples was added to the training dataset".format(newly_added_total))
        
    return latest_additions, latest_additions_filepaths

datasets_bin, added_samples = create_new_dataset(dataset, pseudo_sorted, datasets_bin)

original_unlab_size = DATASET_SIZE_UNLAB
new_unlab_size = DATASET_SIZE_UNLAB - len(added_samples)
original_train_size = int(np.sum(datasets_bin[-2]))
new_train_size = int(np.sum(datasets_bin[-1]))

print("\nFinal Dataset Details:")
print("Newly added Samples: ", len(added_samples))
print("Unlabelled dataset size changed from {} to {} samples".format(original_unlab_size, new_unlab_size))
print("Labelled dataset size changed from {} to {} samples".format(original_train_size, new_train_size))

# Update dataset sizes
DATASET_SIZE_UNLAB -= len(added_samples)
DATASET_SIZE_TRAIN = new_train_size
STEPS_TRAIN = new_train_size//BATCH_SIZE

"""##Use the new dataset to train a pseudo model"""

# Pseudo directory
record_dir = "./records/{}".format("pseudo_model")
pathlib.Path(record_dir).mkdir(parents=True, exist_ok=True) # record_directory

dataset["train"] = resample(dataset["mix_train"], NUMBER_CLASSES)
dataset["train"] = (
  dataset["train"]
  .shuffle(buffer_size=3000, seed=tf.constant(1500, tf.int64))
  .repeat()
  .map(dataset_augmentation(), num_parallel_calls=AUTOTUNE)
  .batch(BATCH_SIZE, drop_remainder=False)
  .prefetch(buffer_size=AUTOTUNE)
)

new_model = model_define()

start_time = time.time()

model_train = model.fit(
    dataset["train"],
    steps_per_epoch = STEPS_TRAIN, 
    epochs = 15,
    validation_data = dataset["val"],
    validation_steps = STEPS_VAL,
    validation_freq = 1,
    verbose = 1
)
print ("Time spent on training: {:.2f} minutes.".format(np.round(time.time() - start_time)/60))

"""### Model Evaluation"""

cd /content/drive/MyDrive/Colab Notebooks (1)/Experiments/InceptionResNetV2+ResNet50/Exp1

model.save('hyperkvasir_InceptionResNetV2+ResNet50_student.h5')

cd /content

true_categories, predicted_categories = calculate_true_and_predicted_cat(model, dataset)
using_evaluation_metrics(model_train, true_categories, predicted_categories)

# Plot the Confusion Matrix
cm = metrics.confusion_matrix(true_categories, predicted_categories)
plot_confusion_matrix(cm, CLASS_NAMES)